# Arquitectura CDC Pipeline - Ecommerce de Autos

## ðŸ“ DiseÃ±o del Sistema

### Arquitectura de Alto Nivel

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ECOMMERCE CDC PIPELINE                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PostgreSQL     â”‚        â”‚   Apache Kafka   â”‚        â”‚   ClickHouse     â”‚
â”‚   (OLTP Source)  â”‚â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  (Event Stream)  â”‚â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  (OLAP Target)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                           â”‚                            â”‚
         â”‚                           â”‚                            â”‚
    Transactions              CDC Events                    Analytics
    (Real-time)              (Streaming)                    (Real-time)
         â”‚                           â”‚                            â”‚
         â–¼                           â–¼                            â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Debezium  â”‚            â”‚  Consumer  â”‚              â”‚Materializedâ”‚
  â”‚  Connector â”‚            â”‚  (Python)  â”‚              â”‚   Views    â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸ”„ Flujo de Datos

### 1. Captura de Cambios (CDC)

```
PostgreSQL WAL â”€â”€â–¶ Debezium â”€â”€â–¶ Kafka Topics
                      â”‚
                      â”œâ”€ CREATE (INSERT)
                      â”œâ”€ UPDATE
                      â””â”€ DELETE
```

**Debezium Configuration:**
- Plugin: `pgoutput` (native PostgreSQL logical replication)
- Snapshot Mode: `initial` (captura estado inicial)
- Table Filtering: Solo tablas relevantes del ecommerce
- Heartbeat: Cada 10 segundos

### 2. Streaming y Procesamiento

```
Kafka Topics
    â”‚
    â”œâ”€ cdc.ecommerce.*.customers
    â”œâ”€ cdc.ecommerce.*.vehicles
    â”œâ”€ cdc.ecommerce.*.orders
    â”œâ”€ cdc.ecommerce.*.order_items
    â””â”€ cdc.ecommerce.*.payments
    
    â†“
    
Python Consumer
    â”‚
    â”œâ”€ DeserializaciÃ³n JSON
    â”œâ”€ TransformaciÃ³n de datos
    â”œâ”€ Enriquecimiento con metadata
    â””â”€ Batch processing
    
    â†“
    
ClickHouse Loader
```

### 3. Almacenamiento AnalÃ­tico

```
ClickHouse Tables
    â”‚
    â”œâ”€ ReplacingMergeTree Engine
    â”‚  â””â”€ Maneja UPDATE/DELETE automÃ¡ticamente
    â”‚
    â”œâ”€ Partitioning por fecha
    â”‚  â””â”€ Optimiza queries temporales
    â”‚
    â””â”€ Materialized Views
       â”œâ”€ Daily Sales Summary
       â”œâ”€ Vehicle Inventory
       â”œâ”€ Customer Lifetime Value
       â””â”€ Payment Analytics
```

## ðŸ—ï¸ Componentes del Sistema

### PostgreSQL (Source Database)

**PropÃ³sito:** Base de datos transaccional OLTP

**Tablas:**
- `customers` - InformaciÃ³n de clientes
- `vehicles` - Inventario de vehÃ­culos
- `orders` - Ã“rdenes de compra
- `order_items` - Detalle de items
- `payments` - Transacciones de pago

**Configuraciones Clave:**
```sql
wal_level = logical
max_wal_senders = 10
max_replication_slots = 10
```

### Debezium CDC Connector

**PropÃ³sito:** Captura de cambios en tiempo real

**CaracterÃ­sticas:**
- Captura eventos INSERT, UPDATE, DELETE
- Snapshot inicial de datos existentes
- Fault tolerance con replication slots
- Transformaciones inline (unwrap)

### Apache Kafka

**PropÃ³sito:** Event streaming backbone

**Componentes:**
- **Kafka Broker:** Almacenamiento distribuido de eventos
- **Zookeeper:** CoordinaciÃ³n del cluster
- **Schema Registry:** GestiÃ³n de schemas
- **Kafka Connect:** Framework para connectors

**ConfiguraciÃ³n:**
- Replication Factor: 1 (desarrollo)
- Partitions: 3 por topic
- Retention: 7 dÃ­as

### Python Consumer

**PropÃ³sito:** Procesamiento y transformaciÃ³n de eventos

**Funcionalidades:**
- Consumo de mÃºltiples topics simultÃ¡neamente
- Batch processing para eficiencia
- TransformaciÃ³n de datos CDC a formato ClickHouse
- Error handling y retries
- Logging estructurado

**CaracterÃ­sticas TÃ©cnicas:**
- Confluent Kafka client (librdkafka)
- Procesamiento asÃ­ncrono
- Commit manual de offsets
- Graceful shutdown

### ClickHouse (Target Database)

**PropÃ³sito:** Base de datos analÃ­tica OLAP

**Engine:** ReplacingMergeTree
- Maneja automÃ¡ticamente duplicados
- Usa `updated_at` como versiÃ³n
- OPTIMIZE FINAL para merges

**Optimizaciones:**
- Ãndices columnares
- Particionamiento por mes
- Materialized views para agregaciones
- CompresiÃ³n eficiente

## ðŸ“Š Modelo de Datos

### Schema Evolution

```
PostgreSQL (Normalized)
    â†“
Debezium (CDC Events)
    â†“
Python (Transformation)
    â†“
ClickHouse (Denormalized + Metadata)
```

### Metadata Adicional en ClickHouse

Cada tabla incluye:
```sql
cdc_operation     String      -- INSERT/UPDATE/DELETE
cdc_timestamp     DateTime64  -- Timestamp del cambio
cdc_source_db     String      -- Database origen
cdc_source_table  String      -- Tabla origen
event_time        DateTime    -- Timestamp de procesamiento
```

## ðŸ” Seguridad y Confiabilidad

### Replication Slots
- Garantiza no pÃ©rdida de eventos
- Mantiene estado de consumo
- Recovery automÃ¡tico

### Offset Management
- Commits manuales despuÃ©s de escribir a ClickHouse
- Exactly-once semantics con transacciones

### Error Handling
- Retries exponenciales
- Dead letter queue (futuro)
- Logging completo de errores

## ðŸ“ˆ Escalabilidad

### Horizontal Scaling

**Kafka:**
- Agregar mÃ¡s brokers
- Incrementar particiones por topic

**Python Consumers:**
- MÃºltiples instancias del consumer
- Cada instancia procesa diferentes particiones

**ClickHouse:**
- Sharding por hash de customer_id
- Distributed tables
- Replication para HA

### Performance Optimization

**Batch Processing:**
- Consumer: Batch de 100 mensajes
- ClickHouse: Inserts en bloque

**Partitioning:**
- Por mes en ClickHouse
- Pruning automÃ¡tico de particiones

**Materialized Views:**
- Pre-agregaciones
- Queries sub-segundo

## ðŸ” Monitoreo

### MÃ©tricas Clave

**Debezium:**
- Snapshot status
- Binlog position
- Event lag

**Kafka:**
- Consumer lag
- Throughput
- Partition distribution

**ClickHouse:**
- Insert rate
- Query performance
- Storage size

### Herramientas

- **Kafka UI:** VisualizaciÃ³n de topics y consumer groups
- **Logs estructurados:** JSON logging con structlog
- **Health checks:** Script de validaciÃ³n de componentes

## ðŸŽ¯ Casos de Uso

### 1. Dashboard de Ventas en Tiempo Real
```sql
SELECT 
    toDate(order_date) as date,
    count() as orders,
    sum(final_amount) as revenue
FROM orders
WHERE order_date >= now() - INTERVAL 7 DAY
GROUP BY date
```

### 2. AnÃ¡lisis de Inventario
```sql
SELECT 
    make, model, status,
    count() as count,
    avg(price) as avg_price
FROM vehicles
GROUP BY make, model, status
```

### 3. Customer Lifetime Value
```sql
SELECT 
    c.customer_id,
    c.email,
    count(o.order_id) as total_orders,
    sum(o.final_amount) as lifetime_value
FROM customers c
JOIN orders o ON c.customer_id = o.customer_id
GROUP BY c.customer_id, c.email
ORDER BY lifetime_value DESC
LIMIT 100
```

## ðŸš€ PrÃ³ximos Pasos

### Mejoras Futuras

1. **Stream Processing con Apache Flink**
   - Complex event processing
   - Windowing avanzado
   - Stateful transformations

2. **Data Quality Monitoring**
   - Great Expectations
   - Anomaly detection
   - Data lineage

3. **Real-time Dashboards**
   - Grafana/Superset
   - WebSocket APIs
   - Streaming visualizations

4. **Machine Learning Integration**
   - Predictive analytics
   - Recommendation engine
   - Fraud detection

5. **Multi-region Replication**
   - Kafka MirrorMaker
   - ClickHouse replication
   - Geo-distribution
